{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 -- Applying Machine Learning Algorithms\n",
    "\n",
    "You will be given a data set, and will apply the techniques we have studied in class to predict a numeric response variable, and to evaluate alternative solutions.\n",
    "\n",
    "The steps will be\n",
    "\n",
    "1.  Exploratory data analysis and evaluation\n",
    "  * Find and correct outliers and missing values\n",
    "  * Find nonlinear relationships between the independent variables and the dependent variable, and transform the input appropriately\n",
    "  * Find correlation in the independent variables and decide how if at all to address it\n",
    "  \n",
    "\n",
    "2. Apply learning techniques.  In each case you will train the algorithm and evaluate it using the *test* $R^2$ statistic.  You will explore different hyperparameter values to find the model you think will maximize test $R^2$ for an evaluation data set.  You will do this for\n",
    "  * Linear regression exploring different variable sets using regular stepwise regression, Lasso, and Ridge Regression\n",
    "  * Decision tree regression exploring different tree depths\n",
    "  * Random forests and boosting exploring different parameter sets\n",
    "  * Neural networks \n",
    "\n",
    "\n",
    "3. Choosing the best method.  You will choose one algorithm and parameter settings you are most confident with, and write a function that enables it to evaluate a new data set.\n",
    "\n",
    "4. When I evaluate your solution, I will call this function on a new set of data, and score your solution (partially) on its results\n",
    "\n",
    "Every part of this assignment has been covered in the notebooks we have looked at in class, so that should be your first source of information and inspiration. \n",
    "\n",
    "<b><span style=\"color: blue\">Cells in blue indicate you should fill in your results -- either text or code.</span></b>\n",
    "\n",
    "When you submit your code, please fill in the cells asked for, but do not add new cells or change the other cells.\n",
    "\n",
    "I will run the cells in your submitted notebook in sequence, so make sure things are in the proper order, all the needed libraries have been imported, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "#### Note on your prediction functions\n",
    "\n",
    "You will notice that for each technique, you are asked to provide a \"prediction\" function that takes an **X** matrix as input.  This **X** matrix will be in the format of the original data set you loaded.  So if in your data cleaning phase you added or deleted or transformed column values, each of these prediction functions must make the same transformations on its input prior to calling you model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "\n",
    "### Loading and Cleaning the Data Set\n",
    "\n",
    "The data set is in a file named **data_set.csv** -- it has 11 independent variables -- some are numeric and some categorical -- and a single numeric response variable $y$.\n",
    "\n",
    "In the first cleaning / analysis phase you should do the following\n",
    "1. Look for outlier values.  When you find outliers, you can do one of two things\n",
    "  * Throw away the data row altogether.  If many variables in the row seem uncommon, it is probably best to delete the row.\n",
    "  * Replace the outlier with a \"reasonable\" value -- probably the mean or median value for that variable.   The reasoning is that the benefit of keeping the row outweighs the error introduced by having a made-up value in one variable\n",
    "2. Look for missing values.  Most algorithms will throw away data rows that have *any* missing value.  You can just delete the row (especially if many values are missing) or assign it a \"reasonable\" value -- probably the mean value for that variable.  If there is an attribute that has many missing values, it is probably best to delete the whole column\n",
    "3. Look for nonlinear relationships.   Most important is finding nonlinear relationships between one of the $x$ variables, and the $y$ variable.  For example, maybe $y$ depends on $x^2$ or $\\log(x)$ rather than on $x$. In that case you need to guess at that relationship, and replace $x$ with a transformed value.  For example if it looks like $y$ depends on $x^2$ then just a column of $x^2$ values.  The easiest way to see these relationships is to do a pair plot between your X variables and y, including a trend line.  \"Well behaved\" $x$ variables tend to show no pattern except for (roughly) following the trend line.  If you are seeing other shapes, or sudden jumps in the behavior of $y$ as $x$ changes, something nonlinear is going on.\n",
    "4. Look for correlations among the $x$ variables.  If you find a correlation you may want to delete one of the correlated variables, but it is not necessary -- you will have to experiment to see if it improves your predictions.  To find correlations, you can use the pair plot, or a correlation matrix, or a heatmap -- there are examples of all of these in the class notebooks.\n",
    "5. Transform categorical variables to dummy (0/1 coded) variables\n",
    "\n",
    "The result of this phase should be a matrix ${\\bf X}$ and a vector ${\\bf y}$ that comprise your training set.\n",
    "\n",
    "Remember though, if you ever need to use your learned function to a new ${\\bf X}$ data set, you need to transform the input ${\\bf X}$ matrix the same way your transformed your test set -- otherwise your learned function will give bad results.\n",
    "\n",
    "Although it looks like this cleaning phase happens before any analysis/learning, the two processes are interleaved.  Start with a simple linear regression model and do minimal cleaning on your data set, just to the point the LR model works.  Then you can test more subtle transformations to see if they make your models perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data cleaning code goes here.  The result should be X and y matrices that can be used to run the models you build below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns #statistic/visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of rows with invalid values\n",
    "def correctDummies(df):\n",
    "    df['x4'] = df['x4'].replace('f' , 'false')\n",
    "    df['x4'] = df['x4'].replace('t', 'true')\n",
    "    df['x4'] = df['x4'].replace('rue', 'true')\n",
    "    df['x3'] = df['x3'].fillna((df['x3']).mean()) # replace all missing values in x3 with mean\n",
    "    df = df.dropna(subset = ['x9']) # too many na's in x9 so drop all of them\n",
    "    df = df[df.x2.isin(['r', 'b', 'y'])]\n",
    "    return df\n",
    "\n",
    "#  Add the dummy variables and get rid of x2 and x4.   Has to be done after correctDummies!\n",
    "def convertDummies(df):\n",
    "    dfnew = df.copy()\n",
    "    x2_dummies = pd.get_dummies(dfnew.x2, prefix='x2')\n",
    "    x4_dummies = pd.get_dummies(dfnew.x4, prefix='x4')\n",
    "    \n",
    "    x2_dummies.drop(x2_dummies.columns[0], axis=1, inplace=True)\n",
    "    x4_dummies.drop(x4_dummies.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    dfnew.drop(['x2','x4'], axis=1, inplace=True)\n",
    "    \n",
    "    dfnew = pd.concat([dfnew, x2_dummies], axis=1)\n",
    "    dfnew = pd.concat([dfnew, x4_dummies], axis=1)\n",
    "    return dfnew\n",
    "\n",
    "def deal_with_nonlinearities(df):\n",
    "    df['x5_big'] = df['x5']\n",
    "    df.loc[df['x5'] <= 410, 'x5_big'] = 0.0\n",
    "    df['x5_small'] = df['x5']\n",
    "    df.loc[df['x5'] > 410, 'x5_small'] = 0.0\n",
    "    \n",
    "    df['x8_square'] = df['x8'] * df['x8']\n",
    "    \n",
    "    df.drop(['x5'],axis = 1, inplace = True)\n",
    "    df.drop(['x8'],axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def cleandf(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df = correctDummies(df)\n",
    "    df = convertDummies(df)\n",
    "    df = deal_with_nonlinearities(df)\n",
    "    df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = 'data_set_test.csv'\n",
    "df = pd.read_csv(file_location, dtype={'x4': str})\n",
    "\n",
    "df = cleandf(df)\n",
    "y = df.y\n",
    "X = df.drop(['y'], axis=1, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6317, 13) (6317,)\n",
      "(2106, 13) (2106,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "#### <span style=\"color: blue\">*Your Summary of EDA / Cleaning Phase*</span>\n",
    "\n",
    "<span style=\"color: blue\">\n",
    "\n",
    "*In this markdown cell please write up the transformations you made to the data set, and why you decided to make those transformations.*\n",
    "\n",
    "</span>\n",
    "\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells you will try various learning techniques on your data set.  For each one you will finish with a prediction function for your best model.  For example for linear regression you will define a function **linear_regression_predict(X)** which will produce the predicted $y$ values for your model.  Remember that the **X** argument is un-transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "### Linear Regression\n",
    "\n",
    "In this section you will try linear regression, and also use Lasso, Ridge Regression and Forward Stepwise Regression to find the set of variables that give you the best $R^2$ score.   You will produce a markdown summary, then implementations of your four models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color: blue\">Summary of Your Linear Regression Models</span>\n",
    "<span style=\"color: blue\">\n",
    "In this markdown cell, summarize your results in building linear regression models for this data set.\n",
    "For each method (full-model regression, forward stepwise regression, Lasso, and Ridge regression) report on the best model:  the variables in the model, the adjusted $R^2$, the estimated test accuracy, and in the case of Lasso, the optimal $\\alpha$ value.  Can you explain the differences in the structure and performance of the alternative models?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE\n",
      "301724201.8812301\n",
      "0.9571159846180235\n",
      "Sample Test MSE\n",
      "303980141.35998255\n",
      "0.9565744069383201\n"
     ]
    }
   ],
   "source": [
    "# This code cell should define a function linear_regression_predict(x_matrix)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "def linear_regression_predict(xdf):\n",
    "    return lr.predict(xdf)\n",
    "\n",
    "y_train_predicted = linear_regression_predict(X_train)\n",
    "y_test_predicted = linear_regression_predict(X_test)\n",
    "\n",
    "print(\"Training MSE\")\n",
    "print(metrics.mean_squared_error(y_train, y_train_predicted))\n",
    "print(metrics.r2_score(y_train, y_train_predicted))\n",
    "print(\"Sample Test MSE\")\n",
    "print(metrics.mean_squared_error(y_test, y_test_predicted))\n",
    "print(metrics.r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import statsmodels.api as sm\n",
    "\n",
    "def statsmodel_summary(dataset, columns):\n",
    "    X = sm.add_constant(dataset[columns])\n",
    "    Y = dataset['y']\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "statsmodel_summary(df, df.columns)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE\n",
      "303095592.1608861\n",
      "0.9570227360496676\n",
      "Sample Test MSE\n",
      "290299392.08171654\n",
      "0.95830494619132\n"
     ]
    }
   ],
   "source": [
    "# This code cell should define a function stepwise_regression_predict(x_matrix)\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def forward_selected(data, response):\n",
    "    \n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    \n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "                \n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    \n",
    "    formula = \"{} ~ {} + 1\".format(response,' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    \n",
    "    return model,selected\n",
    "\n",
    "best_model,x_vars = forward_selected(df, \"y\")\n",
    "\n",
    "#R2 = 0.957\n",
    "def stepwise_regression_predict(xdf):\n",
    "    return best_model.predict(xdf)\n",
    "\n",
    "y_train_predicted = stepwise_regression_predict(X_train)\n",
    "y_test_predicted = stepwise_regression_predict(X_test)\n",
    "\n",
    "print(\"Training MSE\")\n",
    "print(metrics.mean_squared_error(y_train, y_train_predicted))\n",
    "print(metrics.r2_score(y_train, y_train_predicted))\n",
    "print(\"Sample Test MSE\")\n",
    "print(metrics.mean_squared_error(y_test, y_test_predicted))\n",
    "print(metrics.r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.957\n",
      "Model:                            OLS   Adj. R-squared:                  0.957\n",
      "Method:                 Least Squares   F-statistic:                 1.888e+04\n",
      "Date:                Thu, 21 Mar 2019   Prob (F-statistic):               0.00\n",
      "Time:                        00:14:33   Log-Likelihood:                -94156.\n",
      "No. Observations:                8423   AIC:                         1.883e+05\n",
      "Df Residuals:                    8412   BIC:                         1.884e+05\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -2.283e+05   2.72e+04     -8.391      0.000   -2.82e+05   -1.75e+05\n",
      "x5_big      -426.0185     65.251     -6.529      0.000    -553.927    -298.110\n",
      "x1          1.992e+04     94.373    211.039      0.000    1.97e+04    2.01e+04\n",
      "x8_square     10.0149      0.063    158.518      0.000       9.891      10.139\n",
      "x3            73.8873      0.638    115.752      0.000      72.636      75.139\n",
      "x2_y       -2707.2975    464.322     -5.831      0.000   -3617.483   -1797.112\n",
      "x2_r       -2704.0421    460.718     -5.869      0.000   -3607.162   -1800.922\n",
      "x5_small    -135.3130     66.872     -2.023      0.043    -266.398      -4.228\n",
      "x9           -92.3047     64.109     -1.440      0.150    -217.975      33.365\n",
      "x4_true     -531.4856    377.865     -1.407      0.160   -1272.193     209.222\n",
      "x10          -32.2834     26.818     -1.204      0.229     -84.853      20.286\n",
      "==============================================================================\n",
      "Omnibus:                     2728.469   Durbin-Watson:                   0.600\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            27266.678\n",
      "Skew:                           1.261   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.446   Cond. No.                     6.86e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.86e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def statsmodel_summary(dataset, columns):\n",
    "    X = sm.add_constant(dataset[columns])\n",
    "    Y = dataset['y']\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "statsmodel_summary(df, ['x5_big', 'x1', 'x8_square', 'x3', 'x2_y', 'x2_r', 'x5_small', 'x9',\n",
    "       'x4_true', 'x10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, R2: 0.9584111209498068, Terms: 13\n",
      "Alpha: 0.25, R2: 0.9584111202589931, Terms: 13\n",
      "Alpha: 0.35, R2: 0.9584111195173933, Terms: 13\n",
      "Alpha: 0.5, R2: 0.9584111179834078, Terms: 13\n",
      "Alpha: 0.65, R2: 0.9584111159435195, Terms: 13\n",
      "Alpha: 0.75, R2: 0.9584111143025368, Terms: 13\n",
      "Alpha: 0.85, R2: 0.9584111124367081, Terms: 13\n",
      "Alpha: 0.9, R2: 0.9584111114194765, Terms: 13\n",
      "Alpha: 0.95, R2: 0.9584111103460337, Terms: 13\n",
      "Alpha: 0.99, R2: 0.9584111094468071, Terms: 13\n",
      "Alpha: 1.0, R2: 0.9584111092163793, Terms: 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "for a in [.10, .25, .35, .50, .65, .75, .85, .9, .95, .99, 1.0]:\n",
    "    clf = linear_model.Lasso(alpha=a)\n",
    "    clf.fit(X_test,y_test)\n",
    "    r2 = clf.score(X_test,y_test)\n",
    "    coef_count = len(list(filter(lambda x: (x < -.001) | (x > .001), clf.coef_)))\n",
    "    print(\"Alpha: {}, R2: {}, Terms: {}\".format(a, r2, coef_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell should define a function lasso_predict(x_matrix)\n",
    "from sklearn import linear_model\n",
    "\n",
    "# r2 = 0.9573296825234765\n",
    "def lasso_predict(x_matrix):\n",
    "    clf = linear_model.Lasso(alpha=0.1)\n",
    "    clf.fit(x_matrix, y)\n",
    "    print(clf.score(x_matrix,y))\n",
    "    return clf.predict(x_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.1, R2: 0.9570462859083005, Terms: 13\n",
      "Alpha: 0.25, R2: 0.9570462858654045, Terms: 13\n",
      "Alpha: 0.35, R2: 0.9570462858163961, Terms: 13\n",
      "Alpha: 0.5, R2: 0.9570462857122869, Terms: 13\n",
      "Alpha: 0.65, R2: 0.9570462855714884, Terms: 13\n",
      "Alpha: 0.75, R2: 0.957046285457254, Terms: 13\n",
      "Alpha: 0.85, R2: 0.9570462853267344, Terms: 13\n",
      "Alpha: 0.9, R2: 0.9570462852553704, Terms: 13\n",
      "Alpha: 0.95, R2: 0.9570462851799383, Terms: 13\n",
      "Alpha: 0.99, R2: 0.9570462851166643, Terms: 13\n",
      "Alpha: 1.0, R2: 0.9570462851004392, Terms: 13\n"
     ]
    }
   ],
   "source": [
    "for a in [.10, .25, .35, .50, .65, .75, .85, .9, .95, .99, 1.0]:\n",
    "    clf = linear_model.Ridge(alpha=a)\n",
    "    clf.fit(X_train,y_train)\n",
    "    r2 = clf.score(X_train,y_train)\n",
    "    coef_count = len(list(filter(lambda x: (x < -.001) | (x > .001), clf.coef_)))\n",
    "    print(\"Alpha: {}, R2: {}, Terms: {}\".format(a, r2, coef_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9570274557705538\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cd704aee6d79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# This code cell should define a function ridge_predict(x_matrix)\n",
    "from sklearn import linear_model\n",
    "ridge = linear_model.Ridge(alpha = 0.1)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "#r2 = 0.9573439129264358\n",
    "def ridge_predict(xdf):\n",
    "    return ridge.predict(xdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "### Decision Tree Regressors and Ensemble Methods\n",
    "\n",
    "Here you will build decision tree regression learners, and experiment to optimize algorithm parameters.  You will implement learners for \n",
    "* Decision trees\n",
    "* Random forest\n",
    "* Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue\">Summary of Your Decision Tree and Ensemble Method Models</span>\n",
    "<span style=\"color: blue\">\n",
    "In this markdown cell, summarize your results in building tree-based models for this data set.\n",
    "For each method report on the best model:  the model parameters and the estimated test accuracy.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell should define a function decision_tree_predict(x_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell should define a function random_forest_predict(x_matrix) #typecast with int64 ---  no cast needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell should define a function adaboost_predict(x_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "### Neural Networks\n",
    "\n",
    "For this part you will use the *keras* library to implement a neural net regression function.  You will experiment with the structure of the network to optimze for $R^2$.  Remember that your neural net implements a *predict* method, and you can use *sklearn.metrics.r2_score* to evaluate your model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue\">Summary of Neural Network Solution</span>\n",
    "<span style=\"color: blue\">\n",
    "In this markdown cell, summarize your results in building the neural network predictor, including the estimated test accuracy and the model parameterers\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell should define a function neural_network_predict(x_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Your Work\n",
    "In the following code cell, implement a method best_model_predict(X) where X is the same shape as the original training set in the data file.  I will call this function on a new data set generated by the same function, but not part of the training set.  Use whatever method and parameter settings you think will perform best.   **Remember** the ${\\bf X}$ matrix I will call your predict function with will be like the original data matrix, so if you did any transformations on the data set, you will have to do transformation on this matrix too.  It is guaranteed that the data set I used will not have any missing values or deliberate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code cell should define a function best_model_predict(x_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I will copy code into this cell which will (a) read in the evaluation data frame, \n",
    "## (b) call your predict function, and (c) compute a score for your model on my evaluation data set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
